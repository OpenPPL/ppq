{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      ____  ____  __   ____                    __              __\n",
      "     / __ \\/ __ \\/ /  / __ \\__  ______ _____  / /_____  ____  / /\n",
      "    / /_/ / /_/ / /  / / / / / / / __ `/ __ \\/ __/ __ \\/ __ \\/ /\n",
      "   / ____/ ____/ /__/ /_/ / /_/ / /_/ / / / / /_/ /_/ / /_/ / /\n",
      "  /_/   /_/   /_____\\___\\_\\__,_/\\__,_/_/ /_/\\__/\\____/\\____/_/\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ppq import *\n",
    "from ppq.api import *\n",
    "import torch\n",
    "import cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 消除冗余算子\n",
    "import onnx\n",
    "from onnxsim import simplify\n",
    "\n",
    "onnx_model = onnx.load(f'{os.path.join(cfg.FP32_BASE_PATH, name)}-FP32.onnx') \n",
    "model_simp, check = simplify(onnx_model)   #对onnx模型进行简化，消除冗余算子        \n",
    "assert check, \"Simplified ONNX model could not be validated\"\n",
    "onnx.save(model_simp, f'{os.path.join(cfg.FP32_BASE_PATH, name)}-FP32.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/geng/tinyml/ppq/benchmark/detection/FP32_model/Retinanet-FP32-end-11.onnx\"\n",
    "# model_path = \"/home/geng/tinyml/ppq/benchmark/detection/retinanet.onnx\"\n",
    "input_size = [3,480,640]\n",
    "dataloader = [torch.rand(size=input_size) for _ in range(32)]\n",
    "step = 32\n",
    "config = cfg.PLATFORM_CONFIGS[\"TRT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mopset version is not supported, can not generate dispatching scheme with op Less_293(Less), currently we support only [(12, 16)], however 11 was given.\u001b[0m\n",
      "\u001b[31mopset version is not supported, can not generate dispatching scheme with op Less_357(Less), currently we support only [(12, 16)], however 11 was given.\u001b[0m\n",
      "\u001b[31mopset version is not supported, can not generate dispatching scheme with op Less_421(Less), currently we support only [(12, 16)], however 11 was given.\u001b[0m\n",
      "\u001b[31mopset version is not supported, can not generate dispatching scheme with op Less_485(Less), currently we support only [(12, 16)], however 11 was given.\u001b[0m\n",
      "\u001b[31mopset version is not supported, can not generate dispatching scheme with op Less_549(Less), currently we support only [(12, 16)], however 11 was given.\u001b[0m\n",
      "\u001b[31mopset version is not supported, can not generate dispatching scheme with op Less_691(Less), currently we support only [(12, 16)], however 11 was given.\u001b[0m\n",
      "\u001b[31mopset version is not supported, can not generate dispatching scheme with op Less_737(Less), currently we support only [(12, 16)], however 11 was given.\u001b[0m\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error happens when dealing with operation Identity_0(TargetPlatform.SHAPE_OR_INDEX) - inputs:['onnx::Clip_1463_0'], outputs:['onnx::Clip_1467']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/ppq/lib/python3.8/site-packages/ppq-0.6.5-py3.8.egg/ppq/executor/torch.py:342\u001b[0m, in \u001b[0;36mTorchExecutor.__forward\u001b[0;34m(self, inputs, executing_order, output_names, hooks)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[39mif\u001b[39;00m operation\u001b[39m.\u001b[39mtype \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m platform_dispatching_table:\n\u001b[0;32m--> 342\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    343\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mGraph op: \u001b[39m\u001b[39m{\u001b[39;00moperation\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00moperation\u001b[39m.\u001b[39mtype\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    344\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhas no backend implementation on target platform \u001b[39m\u001b[39m{\u001b[39;00moperation\u001b[39m.\u001b[39mplatform\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    345\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mRegister this op to ppq.executor.base.py and ppq.executor.op first\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    346\u001b[0m operation_forward_func \u001b[39m=\u001b[39m platform_dispatching_table[operation\u001b[39m.\u001b[39mtype]\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Graph op: Identity_0(Identity) has no backend implementation on target platform TargetPlatform.SHAPE_OR_INDEX.Register this op to ppq.executor.base.py and ppq.executor.op first",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/geng/tinyml/ppq/benchmark/detection/demo.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bcad02/home/geng/tinyml/ppq/benchmark/detection/demo.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m ppq_quant_ir \u001b[39m=\u001b[39m quantize_onnx_model(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcad02/home/geng/tinyml/ppq/benchmark/detection/demo.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     onnx_import_file\u001b[39m=\u001b[39;49mmodel_path, calib_dataloader\u001b[39m=\u001b[39;49mdataloader, calib_steps\u001b[39m=\u001b[39;49mstep, \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcad02/home/geng/tinyml/ppq/benchmark/detection/demo.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     setting\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mQuanSetting\u001b[39;49m\u001b[39m\"\u001b[39;49m],input_shape\u001b[39m=\u001b[39;49minput_size, collate_fn\u001b[39m=\u001b[39;49m\u001b[39mlambda\u001b[39;49;00m x: x\u001b[39m.\u001b[39;49mto(cfg\u001b[39m.\u001b[39;49mDEVICE), \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcad02/home/geng/tinyml/ppq/benchmark/detection/demo.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     platform\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mQuantPlatform\u001b[39;49m\u001b[39m\"\u001b[39;49m], do_quantize\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/ppq/lib/python3.8/site-packages/ppq-0.6.5-py3.8.egg/ppq/core/defs.py:54\u001b[0m, in \u001b[0;36mempty_ppq_cache.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m empty_cache() \u001b[39m# torch.cuda.empty_cache might requires a sync of all cuda device.\u001b[39;00m\n\u001b[1;32m     53\u001b[0m gc\u001b[39m.\u001b[39mcollect()  \u001b[39m# empty memory.\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/ppq/lib/python3.8/site-packages/ppq-0.6.5-py3.8.egg/ppq/api/interface.py:285\u001b[0m, in \u001b[0;36mquantize_onnx_model\u001b[0;34m(onnx_import_file, calib_dataloader, calib_steps, input_shape, platform, input_dtype, inputs, setting, collate_fn, device, verbose, do_quantize)\u001b[0m\n\u001b[1;32m    283\u001b[0m executor \u001b[39m=\u001b[39m TorchExecutor(graph\u001b[39m=\u001b[39mquantizer\u001b[39m.\u001b[39m_graph, device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m    284\u001b[0m \u001b[39mif\u001b[39;00m do_quantize:\n\u001b[0;32m--> 285\u001b[0m     quantizer\u001b[39m.\u001b[39;49mquantize(\n\u001b[1;32m    286\u001b[0m         inputs\u001b[39m=\u001b[39;49mdummy_input,\n\u001b[1;32m    287\u001b[0m         calib_dataloader\u001b[39m=\u001b[39;49mcalib_dataloader,\n\u001b[1;32m    288\u001b[0m         executor\u001b[39m=\u001b[39;49mexecutor,\n\u001b[1;32m    289\u001b[0m         setting\u001b[39m=\u001b[39;49msetting,\n\u001b[1;32m    290\u001b[0m         calib_steps\u001b[39m=\u001b[39;49mcalib_steps,\n\u001b[1;32m    291\u001b[0m         collate_fn\u001b[39m=\u001b[39;49mcollate_fn\n\u001b[1;32m    292\u001b[0m     )\n\u001b[1;32m    293\u001b[0m     \u001b[39mif\u001b[39;00m verbose: quantizer\u001b[39m.\u001b[39mreport()\n\u001b[1;32m    294\u001b[0m     \u001b[39mreturn\u001b[39;00m quantizer\u001b[39m.\u001b[39m_graph\n",
      "File \u001b[0;32m~/anaconda3/envs/ppq/lib/python3.8/site-packages/ppq-0.6.5-py3.8.egg/ppq/core/defs.py:54\u001b[0m, in \u001b[0;36mempty_ppq_cache.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m empty_cache() \u001b[39m# torch.cuda.empty_cache might requires a sync of all cuda device.\u001b[39;00m\n\u001b[1;32m     53\u001b[0m gc\u001b[39m.\u001b[39mcollect()  \u001b[39m# empty memory.\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/ppq/lib/python3.8/site-packages/ppq-0.6.5-py3.8.egg/ppq/quantization/quantizer/base.py:64\u001b[0m, in \u001b[0;36mBaseQuantizer.quantize\u001b[0;34m(self, inputs, calib_dataloader, executor, setting, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39m# step - 3, quantize all operation(need meta data.)\u001b[39;00m\n\u001b[1;32m     63\u001b[0m executor\u001b[39m.\u001b[39mload_graph(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph)\n\u001b[0;32m---> 64\u001b[0m executor\u001b[39m.\u001b[39;49mtracing_operation_meta(inputs\u001b[39m=\u001b[39;49minputs)\n\u001b[1;32m     65\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquantize_operations(quantable_operation_types\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquant_operation_types)\n\u001b[1;32m     67\u001b[0m \u001b[39m# quantize operation will modify network structure\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[39m# it is necessary calling self._executor before further execution\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[39m# step - 4, calling graph optimization pipeline\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ppq/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/ppq/lib/python3.8/site-packages/ppq-0.6.5-py3.8.egg/ppq/core/defs.py:54\u001b[0m, in \u001b[0;36mempty_ppq_cache.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m empty_cache() \u001b[39m# torch.cuda.empty_cache might requires a sync of all cuda device.\u001b[39;00m\n\u001b[1;32m     53\u001b[0m gc\u001b[39m.\u001b[39mcollect()  \u001b[39m# empty memory.\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/ppq/lib/python3.8/site-packages/ppq-0.6.5-py3.8.egg/ppq/executor/torch.py:434\u001b[0m, in \u001b[0;36mTorchExecutor.tracing_operation_meta\u001b[0;34m(self, inputs, output_names)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[39mfor\u001b[39;00m op_name, operation \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph\u001b[39m.\u001b[39moperations\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    432\u001b[0m     hooks[op_name] \u001b[39m=\u001b[39m TorchMetaDataTracingHook(operation\u001b[39m=\u001b[39moperation)\n\u001b[0;32m--> 434\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__forward(\n\u001b[1;32m    435\u001b[0m     inputs\u001b[39m=\u001b[39;49minputs,\n\u001b[1;32m    436\u001b[0m     output_names\u001b[39m=\u001b[39;49moutput_names,\n\u001b[1;32m    437\u001b[0m     executing_order\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_executing_order,\n\u001b[1;32m    438\u001b[0m     hooks\u001b[39m=\u001b[39;49mhooks)\n\u001b[1;32m    440\u001b[0m \u001b[39mfor\u001b[39;00m op_name, operation \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph\u001b[39m.\u001b[39moperations\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    441\u001b[0m     operation\u001b[39m.\u001b[39mmeta_data \u001b[39m=\u001b[39m OperationMeta(\n\u001b[1;32m    442\u001b[0m         input_metas     \u001b[39m=\u001b[39m hooks[op_name]\u001b[39m.\u001b[39minput_metas,\n\u001b[1;32m    443\u001b[0m         output_metas    \u001b[39m=\u001b[39m hooks[op_name]\u001b[39m.\u001b[39moutput_metas,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    446\u001b[0m         executing_order \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_executing_order\u001b[39m.\u001b[39mindex(operation)\n\u001b[1;32m    447\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/ppq/lib/python3.8/site-packages/ppq-0.6.5-py3.8.egg/ppq/executor/torch.py:399\u001b[0m, in \u001b[0;36mTorchExecutor.__forward\u001b[0;34m(self, inputs, executing_order, output_names, hooks)\u001b[0m\n\u001b[1;32m    396\u001b[0m             result_collector[output_names\u001b[39m.\u001b[39mindex(output_var\u001b[39m.\u001b[39mname)] \u001b[39m=\u001b[39m outputs[output_idx]\n\u001b[1;32m    398\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m _:\n\u001b[0;32m--> 399\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mError happens when dealing with operation \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(operation)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    401\u001b[0m \u001b[39m# remove useless value(runtime clear).\u001b[39;00m\n\u001b[1;32m    402\u001b[0m visited_op\u001b[39m.\u001b[39mappend(operation)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error happens when dealing with operation Identity_0(TargetPlatform.SHAPE_OR_INDEX) - inputs:['onnx::Clip_1463_0'], outputs:['onnx::Clip_1467']"
     ]
    }
   ],
   "source": [
    "ppq_quant_ir = quantize_onnx_model(\n",
    "    onnx_import_file=model_path, calib_dataloader=dataloader, calib_steps=step, \n",
    "    setting=config[\"QuanSetting\"],input_shape=input_size, collate_fn=lambda x: x.to(cfg.DEVICE), \n",
    "    platform=config[\"QuantPlatform\"], do_quantize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "op Less need opset (12,16)\n",
    "op TopK need opset (1,12)\n",
    "Upsample not supported\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ppq')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e1f49b93815805eca9ccc5299d655a62f3a8d0678e274dc3dfeb518f21176dcd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
