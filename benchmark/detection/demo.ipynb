{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      ____  ____  __   ____                    __              __\n",
      "     / __ \\/ __ \\/ /  / __ \\__  ______ _____  / /_____  ____  / /\n",
      "    / /_/ / /_/ / /  / / / / / / / __ `/ __ \\/ __/ __ \\/ __ \\/ /\n",
      "   / ____/ ____/ /__/ /_/ / /_/ / /_/ / / / / /_/ /_/ / /_/ / /\n",
      "  /_/   /_/   /_____\\___\\_\\__,_/\\__,_/_/ /_/\\__/\\____/\\____/_/\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ppq import *\n",
    "from ppq.api import *\n",
    "import torch\n",
    "import cfg\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from turtle import width\n",
    "import onnx\n",
    "import onnx.checker\n",
    "import onnx.utils\n",
    "from onnx.tools import update_model_dims\n",
    "\n",
    "model = onnx.load('/home/geng/tinyml/ppq/benchmark/detection/FP32_model/Retinanet-FP32.onnx')\n",
    "# 此处可以理解为获得了一个维度 “引用”，通过该 “引用“可以修改其对应的维度  \n",
    "                                                                                     \n",
    "height = model.graph.input[0].type.tensor_type.shape.dim[2]\n",
    "# 将该维度赋值为字符串，其维度不再为和dummy_input绑定的值\n",
    "height.dim_param = 'height'\n",
    "\n",
    "width = model.graph.input[0].type.tensor_type.shape.dim[3]\n",
    "# 将该维度赋值为字符串，其维度不再为和dummy_input绑定的值\n",
    "width.dim_param = 'width'\n",
    "\n",
    "onnx.save(model, '/home/geng/tinyml/ppq/benchmark/detection/FP32_model/Retinanet-dynamic-FP32.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 消除冗余算子\n",
    "# import onnx\n",
    "# from onnxsim import simplify\n",
    "\n",
    "# onnx_file = \"/home/geng/tinyml/ppq/benchmark/detection/FP32_model/Retinanet-FP32-end-11.onnx\"\n",
    "# onnx_model = onnx.load(onnx_file) \n",
    "# model_simp, check = simplify(onnx_model)   #对onnx模型进行简化，消除冗余算子        \n",
    "# assert check, \"Simplified ONNX model could not be validated\"\n",
    "# onnx.save(model_simp, onnx_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 升级模型\n",
    "import onnx\n",
    "from onnx import version_converter, helper\n",
    "\n",
    "# # Preprocessing: load the model to be converted.\n",
    "# model_path = \"/home/geng/tinyml/ppq/benchmark/detection/FP32_model/Retinanet-FP32.onnx\"\n",
    "# original_model = onnx.load(model_path)\n",
    "\n",
    "# # A full list of supported adapters can be found here:\n",
    "# # https://github.com/onnx/onnx/blob/main/onnx/version_converter.py#L21\n",
    "# # Apply the version conversion on the original model\n",
    "# converted_model = version_converter.convert_version(original_model, 11)\n",
    "\n",
    "# onnx.save(converted_model, \"/home/geng/tinyml/ppq/benchmark/detection/FP32_model/Retinanet-FP32.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.82s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/home/geng/tinyml/ppq/benchmark/detection/FP32_model/Retinanet-FP32.onnx\"\n",
    "# model_path = \"/home/geng/tinyml/ppq/benchmark/detection/FP32_model/end2end-12.onnx\"\n",
    "\n",
    "from dataset import build_dataset\n",
    "import torch\n",
    "import cfg\n",
    "\n",
    "model_name = \"Retinanet\"\n",
    "input_size = cfg.MODELS[model_name][\"INPUT_SHAPE\"]\n",
    "# 获取校准数据集\n",
    "_,calib_dataloader =  build_dataset(ann_file=cfg.ANN_PATH,data_root=cfg.DATA_ROOT,\n",
    "        input_size=input_size,batch_size=cfg.CALIBRATION_BATCH_SIZE)\n",
    "\n",
    "# calib_dataloader = [torch.rand(size=input_size) for _ in range(512)]\n",
    "\n",
    "\n",
    "config = cfg.PLATFORM_CONFIGS[\"TRT\"]\n",
    "config[\"QuanSetting\"].dispatcher = \"conservative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:29:18] PPQ Quantization Config Refine Pass Running ... Finished.\n",
      "[18:29:18] PPQ Quantization Fusion Pass Running ...        Finished.\n",
      "[18:29:19] PPQ Quantize Simplify Pass Running ...          Finished.\n",
      "[18:29:19] PPQ Parameter Quantization Pass Running ...     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calibration Progress(Phase 1):   0%|          | 0/512 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished.\n",
      "[18:29:19] PPQ Runtime Calibration Pass Running ...        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calibration Progress(Phase 1):   0%|          | 1/512 [00:07<1:04:11,  7.54s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/geng/tinyml/ppq/benchmark/detection/demo.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bcad02/home/geng/tinyml/ppq/benchmark/detection/demo.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m ppq_quant_ir \u001b[39m=\u001b[39m quantize_onnx_model(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcad02/home/geng/tinyml/ppq/benchmark/detection/demo.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     onnx_import_file\u001b[39m=\u001b[39;49mmodel_path, calib_dataloader\u001b[39m=\u001b[39;49mcalib_dataloader, calib_steps\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mCALIBRATION_NUM \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m cfg\u001b[39m.\u001b[39;49mCALIBRATION_BATCH_SIZE, \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcad02/home/geng/tinyml/ppq/benchmark/detection/demo.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     setting\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mQuanSetting\u001b[39;49m\u001b[39m\"\u001b[39;49m],input_shape\u001b[39m=\u001b[39;49minput_size, collate_fn\u001b[39m=\u001b[39;49m\u001b[39mlambda\u001b[39;49;00m x: x[\u001b[39m\"\u001b[39;49m\u001b[39mimg\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mto(cfg\u001b[39m.\u001b[39;49mDEVICE), \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcad02/home/geng/tinyml/ppq/benchmark/detection/demo.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     platform\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mQuantPlatform\u001b[39;49m\u001b[39m\"\u001b[39;49m], do_quantize\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/ppq/lib/python3.8/site-packages/ppq-0.6.5.1-py3.8.egg/ppq/core/defs.py:54\u001b[0m, in \u001b[0;36mempty_ppq_cache.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m empty_cache() \u001b[39m# torch.cuda.empty_cache might requires a sync of all cuda device.\u001b[39;00m\n\u001b[1;32m     53\u001b[0m gc\u001b[39m.\u001b[39mcollect()  \u001b[39m# empty memory.\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/ppq/lib/python3.8/site-packages/ppq-0.6.5.1-py3.8.egg/ppq/api/interface.py:284\u001b[0m, in \u001b[0;36mquantize_onnx_model\u001b[0;34m(onnx_import_file, calib_dataloader, calib_steps, input_shape, platform, input_dtype, inputs, setting, collate_fn, device, verbose, do_quantize)\u001b[0m\n\u001b[1;32m    282\u001b[0m executor \u001b[39m=\u001b[39m TorchExecutor(graph\u001b[39m=\u001b[39mquantizer\u001b[39m.\u001b[39m_graph, device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m    283\u001b[0m \u001b[39mif\u001b[39;00m do_quantize:\n\u001b[0;32m--> 284\u001b[0m     quantizer\u001b[39m.\u001b[39;49mquantize(\n\u001b[1;32m    285\u001b[0m         inputs\u001b[39m=\u001b[39;49mdummy_input,\n\u001b[1;32m    286\u001b[0m         calib_dataloader\u001b[39m=\u001b[39;49mcalib_dataloader,\n\u001b[1;32m    287\u001b[0m         executor\u001b[39m=\u001b[39;49mexecutor,\n\u001b[1;32m    288\u001b[0m         setting\u001b[39m=\u001b[39;49msetting,\n\u001b[1;32m    289\u001b[0m         calib_steps\u001b[39m=\u001b[39;49mcalib_steps,\n\u001b[1;32m    290\u001b[0m         collate_fn\u001b[39m=\u001b[39;49mcollate_fn\n\u001b[1;32m    291\u001b[0m     )\n\u001b[1;32m    292\u001b[0m     \u001b[39mif\u001b[39;00m verbose: quantizer\u001b[39m.\u001b[39mreport()\n\u001b[1;32m    293\u001b[0m     \u001b[39mreturn\u001b[39;00m quantizer\u001b[39m.\u001b[39m_graph\n",
      "File \u001b[0;32m~/anaconda3/envs/ppq/lib/python3.8/site-packages/ppq-0.6.5.1-py3.8.egg/ppq/core/defs.py:54\u001b[0m, in \u001b[0;36mempty_ppq_cache.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m empty_cache() \u001b[39m# torch.cuda.empty_cache might requires a sync of all cuda device.\u001b[39;00m\n\u001b[1;32m     53\u001b[0m gc\u001b[39m.\u001b[39mcollect()  \u001b[39m# empty memory.\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/ppq/lib/python3.8/site-packages/ppq-0.6.5.1-py3.8.egg/ppq/quantization/quantizer/base.py:66\u001b[0m, in \u001b[0;36mBaseQuantizer.quantize\u001b[0;34m(self, inputs, calib_dataloader, executor, setting, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m executor\u001b[39m.\u001b[39mload_graph(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph)\n\u001b[1;32m     64\u001b[0m quant_pipeline \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuild_quant_pipeline(setting)\n\u001b[0;32m---> 66\u001b[0m quant_pipeline\u001b[39m.\u001b[39;49moptimize(\n\u001b[1;32m     67\u001b[0m     graph\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_graph,\n\u001b[1;32m     68\u001b[0m     dataloader\u001b[39m=\u001b[39;49mcalib_dataloader,\n\u001b[1;32m     69\u001b[0m     executor\u001b[39m=\u001b[39;49mexecutor,\n\u001b[1;32m     70\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_verbose,\n\u001b[1;32m     71\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m     72\u001b[0m )\n\u001b[1;32m     74\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_verbose:\n\u001b[1;32m     75\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreport(), end\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/ppq/lib/python3.8/site-packages/ppq-0.6.5.1-py3.8.egg/ppq/quantization/optim/base.py:97\u001b[0m, in \u001b[0;36mQuantizationOptimizationPipeline.optimize\u001b[0;34m(self, graph, dataloader, executor, verbose, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(graph, BaseGraph): \n\u001b[1;32m     95\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mparameter 1 should be an instance of PPQ BaseGraph when calling optim pass, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     96\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhowever \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(graph)\u001b[39m}\u001b[39;00m\u001b[39m was given.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 97\u001b[0m optim_pass\u001b[39m.\u001b[39;49mapply(graph\u001b[39m=\u001b[39;49mgraph, dataloader\u001b[39m=\u001b[39;49mdataloader, executor\u001b[39m=\u001b[39;49mexecutor, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     98\u001b[0m \u001b[39mif\u001b[39;00m verbose: \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFinished.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/ppq/lib/python3.8/site-packages/ppq-0.6.5.1-py3.8.egg/ppq/quantization/optim/base.py:31\u001b[0m, in \u001b[0;36mQuantizationOptimizationPass.apply\u001b[0;34m(self, graph, dataloader, executor, **kwargs)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(graph, BaseGraph):\n\u001b[1;32m     28\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m     29\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mIncorrect graph object input, expect PPQ BaseGraph here, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     30\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mwhile \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(graph)\u001b[39m}\u001b[39;00m\u001b[39m was given.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimize(graph, dataloader\u001b[39m=\u001b[39;49mdataloader, executor\u001b[39m=\u001b[39;49mexecutor, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/ppq/lib/python3.8/site-packages/ppq-0.6.5.1-py3.8.egg/ppq/core/defs.py:54\u001b[0m, in \u001b[0;36mempty_ppq_cache.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m empty_cache() \u001b[39m# torch.cuda.empty_cache might requires a sync of all cuda device.\u001b[39;00m\n\u001b[1;32m     53\u001b[0m gc\u001b[39m.\u001b[39mcollect()  \u001b[39m# empty memory.\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/ppq/lib/python3.8/site-packages/ppq-0.6.5.1-py3.8.egg/ppq/quantization/optim/calibration.py:181\u001b[0m, in \u001b[0;36mRuntimeCalibrationPass.optimize\u001b[0;34m(self, graph, dataloader, executor, calib_steps, collate_fn, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m     hooks[op_name]           \u001b[39m=\u001b[39m observer\u001b[39m.\u001b[39mhook\n\u001b[1;32m    179\u001b[0m \u001b[39m# ready for calibration\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[39m# hook forward function, let observers take effects.\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcalibrate(desc\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mCalibration Progress(Phase 1)\u001b[39;49m\u001b[39m'\u001b[39;49m, dataloader\u001b[39m=\u001b[39;49mdataloader,\n\u001b[1;32m    182\u001b[0m     executor\u001b[39m=\u001b[39;49mexecutor, hooks\u001b[39m=\u001b[39;49mhooks, output_names\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    184\u001b[0m \u001b[39m# render calibration result.\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[39mfor\u001b[39;00m _, observer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_observers\u001b[39m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/anaconda3/envs/ppq/lib/python3.8/site-packages/ppq-0.6.5.1-py3.8.egg/ppq/quantization/optim/calibration.py:123\u001b[0m, in \u001b[0;36mRuntimeCalibrationPass.calibrate\u001b[0;34m(self, desc, dataloader, executor, hooks, output_names)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_collate_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    122\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_collate_fn(data)\n\u001b[0;32m--> 123\u001b[0m executor\u001b[39m.\u001b[39;49mforward(inputs\u001b[39m=\u001b[39;49mdata, hooks\u001b[39m=\u001b[39;49mhooks,\n\u001b[1;32m    124\u001b[0m     output_names\u001b[39m=\u001b[39;49moutput_names)\n\u001b[1;32m    125\u001b[0m progressing_bar\u001b[39m.\u001b[39mupdate()\n\u001b[1;32m    126\u001b[0m calib_step \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/ppq/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/ppq/lib/python3.8/site-packages/ppq-0.6.5.1-py3.8.egg/ppq/executor/torch.py:240\u001b[0m, in \u001b[0;36mTorchExecutor.forward\u001b[0;34m(self, inputs, output_names, hooks)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[39m@\u001b[39m torch\u001b[39m.\u001b[39mno_grad()\n\u001b[1;32m    201\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    202\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    205\u001b[0m     hooks: Dict[\u001b[39mstr\u001b[39m, RuntimeHook] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    206\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    207\u001b[0m     \u001b[39m\"\"\"Forward function of this executor.\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \n\u001b[1;32m    209\u001b[0m \u001b[39m    Notice this forward function will never store and compute gradients.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[39m        List[torch.Tensor]: [executing result, list of tensor objects.]\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__forward(\n\u001b[1;32m    241\u001b[0m         inputs\u001b[39m=\u001b[39;49minputs,\n\u001b[1;32m    242\u001b[0m         output_names\u001b[39m=\u001b[39;49moutput_names,\n\u001b[1;32m    243\u001b[0m         executing_order\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_executing_order,\n\u001b[1;32m    244\u001b[0m         hooks\u001b[39m=\u001b[39;49mhooks\n\u001b[1;32m    245\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/ppq/lib/python3.8/site-packages/ppq-0.6.5.1-py3.8.egg/ppq/executor/torch.py:371\u001b[0m, in \u001b[0;36mTorchExecutor.__forward\u001b[0;34m(self, inputs, executing_order, output_names, hooks)\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[39melse\u001b[39;00m: \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39minvalid hook instance was given with operation: \u001b[39m\u001b[39m{\u001b[39;00moperation\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    370\u001b[0m \u001b[39m# forward and collecting result\u001b[39;00m\n\u001b[0;32m--> 371\u001b[0m outputs \u001b[39m=\u001b[39m operation_forward_func(operation, inputs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_executing_context)\n\u001b[1;32m    372\u001b[0m outputs \u001b[39m=\u001b[39m outputs \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(outputs, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)) \u001b[39melse\u001b[39;00m [outputs]\n\u001b[1;32m    373\u001b[0m fp_outputs \u001b[39m=\u001b[39m outputs\n",
      "File \u001b[0;32m~/anaconda3/envs/ppq/lib/python3.8/site-packages/ppq-0.6.5.1-py3.8.egg/ppq/executor/op/torch/default.py:1623\u001b[0m, in \u001b[0;36mTopK_forward\u001b[0;34m(op, values, ctx, **kwargs)\u001b[0m\n\u001b[1;32m   1621\u001b[0m k \u001b[39m=\u001b[39m convert_any_to_python_primary_type(k)\n\u001b[1;32m   1622\u001b[0m values, indices \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtopk(\u001b[39minput\u001b[39m\u001b[39m=\u001b[39mx, k\u001b[39m=\u001b[39mk, dim\u001b[39m=\u001b[39maxis, largest\u001b[39m=\u001b[39mlargest, \u001b[39msorted\u001b[39m\u001b[39m=\u001b[39m\u001b[39msorted\u001b[39m)\n\u001b[0;32m-> 1623\u001b[0m \u001b[39mreturn\u001b[39;00m values\u001b[39m.\u001b[39;49mto(\u001b[39m'\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m'\u001b[39;49m), indices\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ppq_quant_ir = quantize_onnx_model(\n",
    "    onnx_import_file=model_path, calib_dataloader=calib_dataloader, calib_steps=8 // cfg.CALIBRATION_BATCH_SIZE, \n",
    "    setting=config[\"QuanSetting\"],input_shape=input_size, collate_fn=lambda x: x[\"img\"][0].to(cfg.DEVICE), \n",
    "    platform=config[\"QuantPlatform\"], do_quantize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error happens when dealing with operation Conv_37(TargetPlatform.TRT_INT8) - inputs:['input', 'onnx::Conv_1245', 'onnx::Conv_1246'], outputs:['input.3']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/ppq/lib/python3.8/site-packages/ppq-0.6.5.1-py3.8.egg/ppq/executor/torch.py:371\u001b[0m, in \u001b[0;36mTorchExecutor.__forward\u001b[0;34m(self, inputs, executing_order, output_names, hooks)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[39m# forward and collecting result\u001b[39;00m\n\u001b[0;32m--> 371\u001b[0m outputs \u001b[39m=\u001b[39m operation_forward_func(operation, inputs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_executing_context)\n\u001b[1;32m    372\u001b[0m outputs \u001b[39m=\u001b[39m outputs \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(outputs, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)) \u001b[39melse\u001b[39;00m [outputs]\n",
      "File \u001b[0;32m~/anaconda3/envs/ppq/lib/python3.8/site-packages/ppq-0.6.5.1-py3.8.egg/ppq/executor/op/torch/default.py:215\u001b[0m, in \u001b[0;36mConv_forward\u001b[0;34m(op, values, ctx, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m             onnx_pads \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 215\u001b[0m     output \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mconv2d(\n\u001b[1;32m    216\u001b[0m         \u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49mx, weight\u001b[39m=\u001b[39;49mw, bias\u001b[39m=\u001b[39;49mb, groups\u001b[39m=\u001b[39;49mgroups, padding\u001b[39m=\u001b[39;49monnx_pads,\n\u001b[1;32m    217\u001b[0m         dilation\u001b[39m=\u001b[39;49mdilation, stride\u001b[39m=\u001b[39;49mstride)\n\u001b[1;32m    219\u001b[0m \u001b[39m# conv - 3d\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuDNN error: CUDNN_STATUS_MAPPING_ERROR",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/geng/tinyml/ppq/benchmark/detection/demo.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcad02/home/geng/tinyml/ppq/benchmark/detection/demo.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39minference\u001b[39;00m \u001b[39mimport\u001b[39;00m ppq_inference\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcad02/home/geng/tinyml/ppq/benchmark/detection/demo.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m dataloader \u001b[39m=\u001b[39m [\u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(calib_dataloader))]\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bcad02/home/geng/tinyml/ppq/benchmark/detection/demo.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m outputs \u001b[39m=\u001b[39m ppq_inference(dataloader\u001b[39m=\u001b[39;49mdataloader,ppq_ir \u001b[39m=\u001b[39;49m ppq_quant_ir,device \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/ppq/lib/python3.8/site-packages/ppq-0.6.5.1-py3.8.egg/ppq/core/defs.py:54\u001b[0m, in \u001b[0;36mempty_ppq_cache.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m empty_cache() \u001b[39m# torch.cuda.empty_cache might requires a sync of all cuda device.\u001b[39;00m\n\u001b[1;32m     53\u001b[0m gc\u001b[39m.\u001b[39mcollect()  \u001b[39m# empty memory.\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/tinyml/ppq/benchmark/detection/inference/detection_inference.py:15\u001b[0m, in \u001b[0;36mppq_inference\u001b[0;34m(dataloader, ppq_ir, device)\u001b[0m\n\u001b[1;32m     13\u001b[0m executor \u001b[39m=\u001b[39m TorchExecutor(graph\u001b[39m=\u001b[39mppq_ir, device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m     14\u001b[0m model_forward_function \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m input_tensor: executor(\u001b[39m*\u001b[39m[input_tensor])\n\u001b[0;32m---> 15\u001b[0m \u001b[39mreturn\u001b[39;00m _inference_any_module_with_coco(model_forward_function\u001b[39m=\u001b[39;49mmodel_forward_function,dataloader\u001b[39m=\u001b[39;49mdataloader,device\u001b[39m=\u001b[39;49mdevice)\n",
      "File \u001b[0;32m~/tinyml/ppq/benchmark/detection/inference/detection_inference.py:53\u001b[0m, in \u001b[0;36m_inference_any_module_with_coco\u001b[0;34m(model_forward_function, dataloader, device)\u001b[0m\n\u001b[1;32m     51\u001b[0m input_tensor \u001b[39m=\u001b[39m x[\u001b[39m\"\u001b[39m\u001b[39mimg\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     52\u001b[0m img_metas \u001b[39m=\u001b[39m x[\u001b[39m\"\u001b[39m\u001b[39mimg_metas\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdata[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[0;32m---> 53\u001b[0m output \u001b[39m=\u001b[39m model_forward_function(input_tensor)\n\u001b[1;32m     54\u001b[0m img_metas[\u001b[39m\"\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m output\n\u001b[1;32m     55\u001b[0m outputs\u001b[39m.\u001b[39mappend(img_metas)\n",
      "File \u001b[0;32m~/tinyml/ppq/benchmark/detection/inference/detection_inference.py:14\u001b[0m, in \u001b[0;36mppq_inference.<locals>.<lambda>\u001b[0;34m(input_tensor)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39m@\u001b[39m empty_ppq_cache\n\u001b[1;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mppq_inference\u001b[39m(dataloader,ppq_ir,device\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     13\u001b[0m     executor \u001b[39m=\u001b[39m TorchExecutor(graph\u001b[39m=\u001b[39mppq_ir, device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m---> 14\u001b[0m     model_forward_function \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m input_tensor: executor(\u001b[39m*\u001b[39;49m[input_tensor])\n\u001b[1;32m     15\u001b[0m     \u001b[39mreturn\u001b[39;00m _inference_any_module_with_coco(model_forward_function\u001b[39m=\u001b[39mmodel_forward_function,dataloader\u001b[39m=\u001b[39mdataloader,device\u001b[39m=\u001b[39mdevice)\n",
      "File \u001b[0;32m~/anaconda3/envs/ppq/lib/python3.8/site-packages/ppq-0.6.5.1-py3.8.egg/ppq/executor/base.py:195\u001b[0m, in \u001b[0;36mBaseGraphExecutor.__call__\u001b[0;34m(self, inputs, output_names)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\n\u001b[1;32m    191\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    192\u001b[0m     inputs: Union[\u001b[39mdict\u001b[39m, torch\u001b[39m.\u001b[39mTensor],\n\u001b[1;32m    193\u001b[0m     output_names:List[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    194\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> 195\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(inputs\u001b[39m=\u001b[39;49minputs, output_names\u001b[39m=\u001b[39;49moutput_names)\n",
      "File \u001b[0;32m~/anaconda3/envs/ppq/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/ppq/lib/python3.8/site-packages/ppq-0.6.5.1-py3.8.egg/ppq/executor/torch.py:240\u001b[0m, in \u001b[0;36mTorchExecutor.forward\u001b[0;34m(self, inputs, output_names, hooks)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[39m@\u001b[39m torch\u001b[39m.\u001b[39mno_grad()\n\u001b[1;32m    201\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    202\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    205\u001b[0m     hooks: Dict[\u001b[39mstr\u001b[39m, RuntimeHook] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    206\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    207\u001b[0m     \u001b[39m\"\"\"Forward function of this executor.\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \n\u001b[1;32m    209\u001b[0m \u001b[39m    Notice this forward function will never store and compute gradients.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[39m        List[torch.Tensor]: [executing result, list of tensor objects.]\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__forward(\n\u001b[1;32m    241\u001b[0m         inputs\u001b[39m=\u001b[39;49minputs,\n\u001b[1;32m    242\u001b[0m         output_names\u001b[39m=\u001b[39;49moutput_names,\n\u001b[1;32m    243\u001b[0m         executing_order\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_executing_order,\n\u001b[1;32m    244\u001b[0m         hooks\u001b[39m=\u001b[39;49mhooks\n\u001b[1;32m    245\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/ppq/lib/python3.8/site-packages/ppq-0.6.5.1-py3.8.egg/ppq/executor/torch.py:398\u001b[0m, in \u001b[0;36mTorchExecutor.__forward\u001b[0;34m(self, inputs, executing_order, output_names, hooks)\u001b[0m\n\u001b[1;32m    396\u001b[0m             result_collector[output_names\u001b[39m.\u001b[39mindex(output_var\u001b[39m.\u001b[39mname)] \u001b[39m=\u001b[39m outputs[output_idx]\n\u001b[1;32m    397\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m _:\n\u001b[0;32m--> 398\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mError happens when dealing with operation \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(operation)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    400\u001b[0m \u001b[39m# remove useless value(runtime clear).\u001b[39;00m\n\u001b[1;32m    401\u001b[0m visited_op\u001b[39m.\u001b[39mappend(operation)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error happens when dealing with operation Conv_37(TargetPlatform.TRT_INT8) - inputs:['input', 'onnx::Conv_1245', 'onnx::Conv_1246'], outputs:['input.3']"
     ]
    }
   ],
   "source": [
    "from inference import ppq_inference\n",
    "dataloader = [next(iter(calib_dataloader))]\n",
    "outputs = ppq_inference(dataloader=dataloader,ppq_ir = ppq_quant_ir,device = \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(model_type,outputs,class_num):\n",
    "    \"\"\"\n",
    "    将任意模型的输出转换为常规输出\n",
    "    [N,result]  N为样本数\n",
    "    result: [C,[M,5]] 其中C为类别,M为bbox数目，5为左上角、右下角坐标与置信分数。\n",
    "    并且根据尺度进行复原\n",
    "    \"\"\"\n",
    "    if model_type == \"Retinanet\":\n",
    "        print(\"deal with outputs in Retinanet type\")\n",
    "        results = []\n",
    "        for output in outputs:\n",
    "            img_scale = output[\"scale_factor\"] \n",
    "            labels,bboxs = output[\"output\"]\n",
    "            result = [[] for _ in range(class_num)]\n",
    "            if len(labels.shape) > 2:\n",
    "                # labels 和 bbox的输出顺序发生错换，需要调整\n",
    "                bboxs,labels = labels,bboxs\n",
    "                \n",
    "            bboxs,labels = bboxs[0],labels[0]\n",
    "            for i,label in enumerate(labels):\n",
    "                bboxs[i][:4] = bboxs[i][:4] / img_scale  # 进行bbox的尺寸复原\n",
    "                result[label].append(bboxs[i])\n",
    "            results.append(result)\n",
    "        return results\n",
    "    elif model_type == \"fasterrcnn\":\n",
    "        # TODO\n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deal with outputs in Retinanet type\n"
     ]
    }
   ],
   "source": [
    "results = post_process(model_name,outputs,80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Retinanet\"\n",
    "export_ppq_graph(   graph = ppq_quant_ir,\n",
    "                    platform=TargetPlatform.ONNXRUNTIME,\n",
    "                    graph_save_to=f'{os.path.join(config[\"OutputPath\"], model_name)}-ORT-END-INT8.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(calib_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 800, 1216])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[\"img\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 3, 800, 1216)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calib_input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "op Less need opset (12,16)\n",
    "op TopK need opset (1,12)\n",
    "Upsample not supported\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "image_path = \"/home/geng/fiftyone/coco-2017/validation/data/000000061418.jpg\"\n",
    "image = Image.open(image_path)\n",
    "\n",
    "ratio = 800.0 / min(image.size[0], image.size[1])\n",
    "image = image.resize((int(ratio * image.size[0]), int(ratio * image.size[1])),  Image.Resampling.BILINEAR)\n",
    "image = cv2.cvtColor(np.array(image) , cv2.COLOR_GRAY2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 1117, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(image).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 96, 109, 130, ..., 208, 213, 217],\n",
       "       [ 93, 106, 126, ..., 211, 213, 215],\n",
       "       [ 89, 101, 119, ..., 215, 213, 212],\n",
       "       ...,\n",
       "       [116, 117, 119, ...,  94,  94,  93],\n",
       "       [107, 108, 112, ..., 103, 106, 108],\n",
       "       [101, 103, 107, ..., 109, 114, 117]], dtype=uint8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.random.rand(10,15)\n",
    "b = np.random.rand(5,5)\n",
    "a = (a > 0.5).astype(\"int\")\n",
    "b = np.asfortranarray(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[10:10,3:5] = b[0:0,1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(image):\n",
    "    channel_num = image.layers  #通道数\n",
    "\n",
    "    # Resize\n",
    "    ratio = 800.0 / min(image.size[0], image.size[1])\n",
    "    image = image.resize((int(ratio * image.size[0]), int(ratio * image.size[1])),  Image.Resampling.BILINEAR)\n",
    "    \n",
    "    if channel_num == 1:  #灰度图像转三通道\n",
    "        # print(\"trans\")\n",
    "        image = cv2.cvtColor(np.array(image) , cv2.COLOR_GRAY2RGB)\n",
    "   \n",
    "    # Convert to BGR\n",
    "    image = np.array(image)[:, :, [2, 1, 0]].astype('float32')\n",
    "\n",
    "    # HWC -> CHW\n",
    "    image = np.transpose(image, [2, 0, 1])\n",
    "\n",
    "    # Normalize\n",
    "    mean_vec = np.array([102.9801, 115.9465, 122.7717])\n",
    "    for i in range(image.shape[0]):\n",
    "        image[i, :, :] = image[i, :, :] - mean_vec[i]\n",
    "\n",
    "    # Pad to be divisible of 32\n",
    "    import math\n",
    "    padded_h = int(math.ceil(image.shape[1] / 32) * 32)\n",
    "    padded_w = int(math.ceil(image.shape[2] / 32) * 32)\n",
    "\n",
    "    padded_image = np.zeros((3, padded_h, padded_w), dtype=np.float32)\n",
    "    padded_image[:, :image.shape[1], :image.shape[2]] = image\n",
    "    image = padded_image\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      ____  ____  __   ____                    __              __\n",
      "     / __ \\/ __ \\/ /  / __ \\__  ______ _____  / /_____  ____  / /\n",
      "    / /_/ / /_/ / /  / / / / / / / __ `/ __ \\/ __/ __ \\/ __ \\/ /\n",
      "   / ____/ ____/ /__/ /_/ / /_/ / /_/ / / / / /_/ /_/ / /_/ / /\n",
      "  /_/   /_/   /_____\\___\\_\\__,_/\\__,_/_/ /_/\\__/\\____/\\____/_/\n",
      "\n",
      "\n",
      "[09/20/2022-18:10:23] [TRT] [I] [MemUsageChange] Init CUDA: CPU +310, GPU +0, now: CPU 395, GPU 1761 (MiB)\n",
      "[09/20/2022-18:10:23] [TRT] [I] Loaded engine size: 196 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/geng/anaconda3/envs/ppq/lib/python3.8/site-packages/tensorrt/__init__.py:75: DeprecationWarning: Context managers for TensorRT types are deprecated. Memory will be freed automatically when the reference count reaches 0.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09/20/2022-18:10:24] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +496, GPU +214, now: CPU 1111, GPU 2171 (MiB)\n",
      "[09/20/2022-18:10:24] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +392, GPU +200, now: CPU 1503, GPU 2371 (MiB)\n",
      "[09/20/2022-18:10:24] [TRT] [W] TensorRT was linked against cuDNN 8.4.1 but loaded cuDNN 8.2.2\n",
      "[09/20/2022-18:10:24] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +211, now: CPU 0, GPU 211 (MiB)\n",
      "[09/20/2022-18:10:25] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 1306, GPU 2379 (MiB)\n",
      "[09/20/2022-18:10:25] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 1306, GPU 2387 (MiB)\n",
      "[09/20/2022-18:10:25] [TRT] [W] TensorRT was linked against cuDNN 8.4.1 but loaded cuDNN 8.2.2\n",
      "[09/20/2022-18:10:25] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +579, now: CPU 0, GPU 790 (MiB)\n",
      "[09/20/2022-18:10:25] [TRT] [W] The getMaxBatchSize() function should not be used with an engine built from a network created with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag. This function will always return 1.\n",
      "[Host:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Device:\n",
      "<pycuda._driver.DeviceAllocation object at 0x7eff11b2f5e0>, Host:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Device:\n",
      "<pycuda._driver.DeviceAllocation object at 0x7eff11b2f6a0>]\n",
      "[09/20/2022-18:10:25] [TRT] [W] The getMaxBatchSize() function should not be used with an engine built from a network created with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag. This function will always return 1.\n",
      "[09/20/2022-18:10:25] [TRT] [W] The getMaxBatchSize() function should not be used with an engine built from a network created with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag. This function will always return 1.\n",
      "(1, 3, 800, 1216)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/geng/tinyml/ppq/benchmark/detection/inference/trt_infer.py:134: DeprecationWarning: Use network created with NetworkDefinitionCreationFlag::EXPLICIT_BATCH flag instead.\n",
      "  size = trt.volume(engine.get_binding_shape(binding)) * engine.max_batch_size\n"
     ]
    }
   ],
   "source": [
    "from inference import TrtInferenceModel\n",
    "import ctypes\n",
    "import numpy as np\n",
    "\n",
    "ctypes.CDLL(\"/home/geng/tinyml/ppq/benchmark/detection/lib/libmmdeploy_tensorrt_ops.so\")\n",
    "\n",
    "trt_model_path = '/home/geng/tinyml/ppq/benchmark/detection/dynamic_input_test/FasterRCNN-NMS-FP32.engine'\n",
    "trtmodel = TrtInferenceModel(trt_model_path)\n",
    "\n",
    "with open(\"/home/geng/tinyml/ppq/benchmark/detection/images_list.txt\",\"r\") as f:\n",
    "   img_path_list =  f.readlines()\n",
    "img_path_list  = [x.rstrip() for x in img_path_list]\n",
    "img_path = img_path_list[0]\n",
    "\n",
    "from PIL import Image\n",
    "engine_path ='/home/geng/tinyml/ppq/benchmark/detection/dynamic_input_test/FasterRCNN-NMS-FP32.engine'\n",
    "img = Image.open(img_path)\n",
    "input_tensor = preprocess(img)[None,:,:,:]\n",
    "print(input_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "LogicError",
     "evalue": "cuMemcpyHtoDAsync failed: invalid argument",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLogicError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/geng/tinyml/ppq/benchmark/detection/demo.ipynb Cell 21\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bcad02/home/geng/tinyml/ppq/benchmark/detection/demo.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m outputs \u001b[39m=\u001b[39m trtmodel(input_tensor\u001b[39m.\u001b[39;49mastype(np\u001b[39m.\u001b[39;49mfloat32))\n",
      "File \u001b[0;32m~/tinyml/ppq/benchmark/detection/inference/trt_infer.py:272\u001b[0m, in \u001b[0;36mTrtInferenceModel.__call__\u001b[0;34m(self, input_tensor)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m,input_tensor):\n\u001b[1;32m    271\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minputs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mhost \u001b[39m=\u001b[39m convert_any_to_numpy(input_tensor)\n\u001b[0;32m--> 272\u001b[0m     output \u001b[39m=\u001b[39m do_inference_v2(\n\u001b[1;32m    273\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcontext, bindings\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbindings, inputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minputs, \n\u001b[1;32m    274\u001b[0m         outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutputs, stream\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstream, batch_size\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m    275\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/tinyml/ppq/benchmark/detection/inference/trt_infer.py:166\u001b[0m, in \u001b[0;36mdo_inference_v2\u001b[0;34m(context, bindings, inputs, outputs, stream, batch_size)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdo_inference_v2\u001b[39m(context, bindings, inputs, outputs, stream,batch_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m    165\u001b[0m     \u001b[39m# Transfer input data to the GPU.\u001b[39;00m\n\u001b[0;32m--> 166\u001b[0m     [cuda\u001b[39m.\u001b[39mmemcpy_htod_async(inp\u001b[39m.\u001b[39mdevice, inp\u001b[39m.\u001b[39mhost, stream) \u001b[39mfor\u001b[39;00m inp \u001b[39min\u001b[39;00m inputs]\n\u001b[1;32m    167\u001b[0m     \u001b[39m# Run inference.\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     context\u001b[39m.\u001b[39mexecute_async_v2(bindings\u001b[39m=\u001b[39mbindings, stream_handle\u001b[39m=\u001b[39mstream\u001b[39m.\u001b[39mhandle)\n",
      "File \u001b[0;32m~/tinyml/ppq/benchmark/detection/inference/trt_infer.py:166\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdo_inference_v2\u001b[39m(context, bindings, inputs, outputs, stream,batch_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m    165\u001b[0m     \u001b[39m# Transfer input data to the GPU.\u001b[39;00m\n\u001b[0;32m--> 166\u001b[0m     [cuda\u001b[39m.\u001b[39;49mmemcpy_htod_async(inp\u001b[39m.\u001b[39;49mdevice, inp\u001b[39m.\u001b[39;49mhost, stream) \u001b[39mfor\u001b[39;00m inp \u001b[39min\u001b[39;00m inputs]\n\u001b[1;32m    167\u001b[0m     \u001b[39m# Run inference.\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     context\u001b[39m.\u001b[39mexecute_async_v2(bindings\u001b[39m=\u001b[39mbindings, stream_handle\u001b[39m=\u001b[39mstream\u001b[39m.\u001b[39mhandle)\n",
      "\u001b[0;31mLogicError\u001b[0m: cuMemcpyHtoDAsync failed: invalid argument"
     ]
    }
   ],
   "source": [
    "outputs = trtmodel(input_tensor.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "print(f\"正在获取预训练模型：{name}\")\n",
    "dump_torch_to_onnx(model=model, onnx_export_file=f'{os.path.join(cfg.FP32_BASE_PATH, name)}-FP32.onnx',\n",
    "    input_shape=cfg.INPUT_SHAPE,input_dtype=torch.float,device=\"cpu\")\n",
    "\n",
    "onnx_model = onnx.load(f'{os.path.join(cfg.FP32_BASE_PATH, name)}-FP32.onnx') \n",
    "model_simp, check = simplify(onnx_model)   #对onnx模型进行简化，消除冗余算子        \n",
    "assert check, \"Simplified ONNX model could not be validated\"\n",
    "onnx.save(model_simp, f'{os.path.join(cfg.FP32_BASE_PATH, name)}-FP32.onnx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ppq')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e1f49b93815805eca9ccc5299d655a62f3a8d0678e274dc3dfeb518f21176dcd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
