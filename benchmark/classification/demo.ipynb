{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对分类模型进行量化，并测试精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from ppq import *\n",
    "from ppq.api import *\n",
    "from Utilities.Imagenet import (evaluate_mmlab_module_with_imagenet,\n",
    "                                evaluate_onnx_module_with_imagenet,\n",
    "                                evaluate_ppq_module_with_imagenet,\n",
    "                                evaluate_torch_module_with_imagenet,\n",
    "                                load_imagenet_from_directory)\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试全精度模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG_PLATFORM = TargetPlatform.TRT_INT8  # 用来指定目标平台\n",
    "platform = \"TRT\"  #记得修改上面两个\n",
    "QUANT_SETTING = QuantizationSettingFactory.trt_setting() # 用来指定量化配置\n",
    "\n",
    "CFG_DEVICE = 'cuda'                            # 一个神奇的字符串，用来确定执行设备\n",
    "CFG_BATCHSIZE = 64                             # 测试与calib时的 batchsize\n",
    "CFG_INPUT_SHAPE = (CFG_BATCHSIZE, 3, 224, 224) # 用来确定模型输入的尺寸，好像 imagenet 都是这个尺寸\n",
    "CFG_VALIDATION_DIR = '/home/geng/tinyml/ppq/benchmark/Assets/Imagenet_Valid'   # 用来读取 validation dataset\n",
    "CFG_TRAIN_DIR = '/home/geng/tinyml/ppq/benchmark/Assets/Imagenet_Train'        # 用来读取 train dataset，注意该集合将被用来 calibrate 你的模型\n",
    "CFG_DUMP_PATH = '/home/geng/tinyml/ppq/benchmark/classification/'+platform+'_output'    # 所有模型保存的路径名\n",
    "\n",
    "if not os.path.exists(CFG_DUMP_PATH):\n",
    "    os.makedirs(CFG_DUMP_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|model|TargetPlatform|ORT FP32|PPQ INT8|DQD ORT INT8|RealPlatform INT8|\n",
    "|----|----|----|----|----|----|\n",
    "|resnet18|OpenVino|69.764|69.466|67.109|-|\n",
    "|resnet18|TRT|69.764|69.548|69.524|-|\n",
    "|resnet18|Snpe|69.764|69.278|69.266|-|\n",
    "|resnet18|Ncnn|69.764|69.106|69.070|-|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mPPQ is compling CUDA Kernels. Please wait...If there is any problem with kernel compilation, feel free to remove ENABLE_CUDA_KERNEL clause.\u001b[0m\n",
      "---------------------- PPQ Quantization Test Running with resnet18 ----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/geng/anaconda3/envs/ppq/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/geng/anaconda3/envs/ppq/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Evaluating Model...:   0%|          | 0/781 [00:00<?, ?it/s]/home/geng/tinyml/ppq/benchmark/classification/Utilities/Imagenet/imagenet_util.py:188: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  prec1, prec5 = accuracy(torch.tensor(batch_pred).to('cpu'), batch_label.to('cpu'), topk=(1, 5))\n",
      "Evaluating Model...:   0%|          | 1/781 [00:00<08:21,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [0 / 781]\tPrec@1 84.375 (84.375)\tPrec@5 95.312 (95.312)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model...:  13%|█▎        | 104/781 [00:05<00:31, 21.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [100 / 781]\tPrec@1 76.269 (76.269)\tPrec@5 92.280 (92.280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model...:  26%|██▌       | 203/781 [00:10<00:24, 23.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [200 / 781]\tPrec@1 75.793 (75.793)\tPrec@5 93.190 (93.190)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model...:  39%|███▊      | 302/781 [00:15<00:20, 23.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [300 / 781]\tPrec@1 76.230 (76.230)\tPrec@5 93.490 (93.490)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model...:  52%|█████▏    | 404/781 [00:20<00:16, 22.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [400 / 781]\tPrec@1 73.589 (73.589)\tPrec@5 91.732 (91.732)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model...:  65%|██████▍   | 506/781 [00:24<00:11, 24.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [500 / 781]\tPrec@1 72.040 (72.040)\tPrec@5 90.556 (90.556)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model...:  77%|███████▋  | 605/781 [00:29<00:07, 22.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [600 / 781]\tPrec@1 70.793 (70.793)\tPrec@5 89.757 (89.757)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model...:  90%|█████████ | 704/781 [00:33<00:03, 20.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [700 / 781]\tPrec@1 69.824 (69.824)\tPrec@5 89.042 (89.042)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model...: 100%|██████████| 781/781 [00:37<00:00, 20.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Prec@1 69.764 Prec@5 89.085\n",
      "[07:45:02] PPQ Quantization Config Refine Pass Running ... Finished.\n",
      "[07:45:02] PPQ Quantization Fusion Pass Running ...        Finished.\n",
      "[07:45:02] PPQ Quantize Point Reduce Pass Running ...      Finished.\n",
      "[07:45:02] PPQ Parameter Quantization Pass Running ...     Finished.\n",
      "[07:45:02] PPQ Runtime Calibration Pass Running ...        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calibration Progress(Phase 1): 100%|██████████| 80/80 [00:07<00:00, 10.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished.\n",
      "[07:45:10] PPQ Quantization Alignment Pass Running ...     Finished.\n",
      "[07:45:10] PPQ Passive Parameter Quantization Running ...  Finished.\n",
      "[07:45:10] PPQ Parameter Baking Pass Running ...           Finished.\n",
      "--------- Network Snapshot ---------\n",
      "Num of Op:                    [49]\n",
      "Num of Quantized Op:          [47]\n",
      "Num of Variable:              [92]\n",
      "Num of Quantized Var:         [88]\n",
      "------- Quantization Snapshot ------\n",
      "Num of Quant Config:          [142]\n",
      "BAKED:                        [20]\n",
      "OVERLAPPED:                   [56]\n",
      "SLAVE:                        [19]\n",
      "ACTIVATED:                    [27]\n",
      "PASSIVE_BAKED:                [20]\n",
      "Network Quantization Finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model...:   0%|          | 0/781 [00:00<?, ?it/s]/home/geng/tinyml/ppq/benchmark/classification/Utilities/Imagenet/imagenet_util.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  model_forward_function = lambda input_tensor: torch.tensor(\n",
      "/home/geng/tinyml/ppq/benchmark/classification/Utilities/Imagenet/imagenet_util.py:188: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  prec1, prec5 = accuracy(torch.tensor(batch_pred).to('cpu'), batch_label.to('cpu'), topk=(1, 5))\n",
      "Evaluating Model...:   0%|          | 1/781 [00:00<11:45,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [0 / 781]\tPrec@1 85.938 (85.938)\tPrec@5 93.750 (93.750)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model...:  13%|█▎        | 103/781 [00:08<00:48, 14.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [100 / 781]\tPrec@1 75.541 (75.541)\tPrec@5 92.126 (92.126)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model...:  26%|██▌       | 203/781 [00:15<00:41, 14.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [200 / 781]\tPrec@1 75.241 (75.241)\tPrec@5 93.074 (93.074)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model...:  39%|███▉      | 303/781 [00:22<00:33, 14.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [300 / 781]\tPrec@1 75.903 (75.903)\tPrec@5 93.350 (93.350)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model...:  52%|█████▏    | 403/781 [00:29<00:26, 14.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [400 / 781]\tPrec@1 73.231 (73.231)\tPrec@5 91.650 (91.650)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model...:  64%|██████▍   | 503/781 [00:36<00:19, 14.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [500 / 781]\tPrec@1 71.597 (71.597)\tPrec@5 90.472 (90.472)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model...:  77%|███████▋  | 603/781 [00:43<00:12, 13.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [600 / 781]\tPrec@1 70.289 (70.289)\tPrec@5 89.624 (89.624)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model...:  90%|█████████ | 703/781 [00:51<00:05, 14.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [700 / 781]\tPrec@1 69.318 (69.318)\tPrec@5 88.880 (88.880)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model...: 100%|██████████| 781/781 [00:56<00:00, 13.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Prec@1 69.278 Prec@5 88.936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/geng/anaconda3/envs/ppq/lib/python3.8/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:55: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'CPUExecutionProvider'\n",
      "  warnings.warn(\"Specified provider '{}' is not in available provider names.\"\n",
      "Evaluating Model...:   0%|          | 0/781 [00:00<?, ?it/s]/home/geng/tinyml/ppq/benchmark/classification/Utilities/Imagenet/imagenet_util.py:188: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  prec1, prec5 = accuracy(torch.tensor(batch_pred).to('cpu'), batch_label.to('cpu'), topk=(1, 5))\n",
      "Evaluating Model...:   0%|          | 1/781 [00:01<17:37,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [0 / 781]\tPrec@1 85.938 (85.938)\tPrec@5 93.750 (93.750)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model...:  13%|█▎        | 101/781 [00:59<06:21,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [100 / 781]\tPrec@1 75.572 (75.572)\tPrec@5 92.141 (92.141)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model...:  26%|██▌       | 201/781 [01:56<05:43,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [200 / 781]\tPrec@1 75.218 (75.218)\tPrec@5 93.097 (93.097)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model...:  39%|███▊      | 301/781 [02:53<04:39,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [300 / 781]\tPrec@1 75.851 (75.851)\tPrec@5 93.355 (93.355)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model...:  51%|█████▏    | 401/781 [03:52<03:48,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [400 / 781]\tPrec@1 73.200 (73.200)\tPrec@5 91.619 (91.619)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model...:  64%|██████▍   | 501/781 [05:01<02:57,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [500 / 781]\tPrec@1 71.551 (71.551)\tPrec@5 90.447 (90.447)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model...:  77%|███████▋  | 601/781 [06:09<02:05,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [600 / 781]\tPrec@1 70.255 (70.255)\tPrec@5 89.588 (89.588)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model...:  90%|████████▉ | 701/781 [08:25<02:09,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [700 / 781]\tPrec@1 69.307 (69.307)\tPrec@5 88.844 (88.844)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model...: 100%|██████████| 781/781 [10:34<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Prec@1 69.266 Prec@5 88.896\n"
     ]
    }
   ],
   "source": [
    "with ENABLE_CUDA_KERNEL():\n",
    "    model_builder, model_name  = torchvision.models.resnet18, 'resnet18'\n",
    "\n",
    "    print(f'---------------------- PPQ Quantization Test Running with {model_name} ----------------------')\n",
    "    model = model_builder(pretrained=True).to(CFG_DEVICE)\n",
    "\n",
    "    #测试FP32精度\n",
    "    # fp32_report = evaluate_torch_module_with_imagenet(\n",
    "    #     model=model, imagenet_validation_dir=CFG_VALIDATION_DIR,\n",
    "    #     batchsize=CFG_BATCHSIZE, device=CFG_DEVICE, verbose=True)\n",
    "\n",
    "    # 获取校准数据\n",
    "    dataloader = load_imagenet_from_directory(\n",
    "        directory=CFG_TRAIN_DIR, batchsize=CFG_BATCHSIZE,\n",
    "        shuffle=False, subset=5120, require_label=False,\n",
    "        num_of_workers=8)\n",
    "\n",
    "    # 量化torch模型\n",
    "    ppq_quant_ir = quantize_torch_model(\n",
    "        model=model, calib_dataloader=dataloader, input_shape=CFG_INPUT_SHAPE,\n",
    "        calib_steps=5120 // CFG_BATCHSIZE, collate_fn=lambda x: x.to(CFG_DEVICE), verbose=1,\n",
    "        device=CFG_DEVICE, platform=CFG_PLATFORM, setting=QUANT_SETTING,\n",
    "        onnx_export_file=f'{os.path.join(CFG_DUMP_PATH, model_name)}-FP32.onnx')\n",
    "        \n",
    "    # 评估PPQ量化后的模型\n",
    "    ppq_int8_report = evaluate_ppq_module_with_imagenet(\n",
    "        model=ppq_quant_ir, imagenet_validation_dir=CFG_VALIDATION_DIR,\n",
    "        batchsize=CFG_BATCHSIZE, device=CFG_DEVICE, verbose=True)\n",
    "\n",
    "    # 导出ORT模型\n",
    "    export_ppq_graph(\n",
    "        graph=ppq_quant_ir, \n",
    "        platform=TargetPlatform.ONNXRUNTIME,\n",
    "        graph_save_to=f'{os.path.join(CFG_DUMP_PATH, model_name)}-INT8.onnx')\n",
    "    \n",
    "    # 导出部署平台模型\n",
    "    export_ppq_graph(\n",
    "        graph=ppq_quant_ir, \n",
    "        platform=TargetPlatform.TRT_INT8,\n",
    "        graph_save_to=f'{os.path.join(CFG_DUMP_PATH, model_name)}-INT8.onnx')\n",
    "    \n",
    "    # 评估onnx运行模型\n",
    "    evaluate_onnx_module_with_imagenet(\n",
    "        onnxruntime_model_path=f'{os.path.join(CFG_DUMP_PATH, model_name)}-INT8.onnx', \n",
    "        imagenet_validation_dir=CFG_VALIDATION_DIR, batchsize=CFG_BATCHSIZE, \n",
    "        device=CFG_DEVICE)\n",
    "\n",
    "    # ppq_int8_report.to_csv(f'{os.path.join(CFG_DUMP_PATH, model_name)}-report.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估Openvino推理精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      ____  ____  __   ____                    __              __\n",
      "     / __ \\/ __ \\/ /  / __ \\__  ______ _____  / /_____  ____  / /\n",
      "    / /_/ / /_/ / /  / / / / / / / __ `/ __ \\/ __/ __ \\/ __ \\/ /\n",
      "   / ____/ ____/ /__/ /_/ / /_/ / /_/ / / / / /_/ /_/ / /_/ / /\n",
      "  /_/   /_/   /_____\\___\\_\\__,_/\\__,_/_/ /_/\\__/\\____/\\____/_/\n",
      "\n",
      "\n",
      "\u001b[31mTensorRT is not installed, TRT Exporter is disabled.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import openvino\n",
    "import openvino.runtime\n",
    "import cfg\n",
    "import torch\n",
    "from ppq.core import *\n",
    "from Utilities.Imagenet import evaluate_openvino_module_with_imagenet\n",
    "\n",
    "CFG_BATCHSIZE = 64                             # 测试与calib时的 batchsize\n",
    "CFG_INPUT_SHAPE = (CFG_BATCHSIZE, 3, 224, 224) # 用来确定模型输入的尺寸，好像 imagenet 都是这个尺寸\n",
    "CFG_VALIDATION_DIR = '/home/geng/tinyml/ppq/benchmark/Assets/Imagenet_Valid'   # 用来读取 validation dataset\n",
    "CFG_TRAIN_DIR = '/home/geng/tinyml/ppq/benchmark/Assets/Imagenet_Train'        # 用来读取 train dataset，注意该集合将被用来 calibrate 你的模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model...:   0%|          | 0/781 [00:00<?, ?it/s]/home/geng/tinyml/ppq/benchmark/classification/Utilities/Imagenet/imagenet_util.py:213: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  prec1, prec5 = accuracy(torch.tensor(batch_pred).to('cpu'), batch_label.to('cpu'), topk=(1, 5))\n",
      "Evaluating Model...:   0%|          | 1/781 [00:01<13:35,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [0 / 781]\tPrec@1 81.250 (81.250)\tPrec@5 92.188 (92.188)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model...:  13%|█▎        | 101/781 [00:33<03:30,  3.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [100 / 781]\tPrec@1 74.165 (74.165)\tPrec@5 91.352 (91.352)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model...:  26%|██▌       | 201/781 [01:04<03:06,  3.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [200 / 781]\tPrec@1 73.298 (73.298)\tPrec@5 92.141 (92.141)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model...:  39%|███▊      | 301/781 [01:35<02:29,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [300 / 781]\tPrec@1 73.785 (73.785)\tPrec@5 92.229 (92.229)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model...:  51%|█████▏    | 401/781 [02:07<01:57,  3.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [400 / 781]\tPrec@1 71.267 (71.267)\tPrec@5 90.290 (90.290)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model...:  64%|██████▍   | 501/781 [02:38<01:29,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [500 / 781]\tPrec@1 69.586 (69.586)\tPrec@5 88.997 (88.997)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model...:  77%|███████▋  | 601/781 [03:09<00:55,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [600 / 781]\tPrec@1 68.274 (68.274)\tPrec@5 88.025 (88.025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model...:  77%|███████▋  | 605/781 [03:11<00:55,  3.16it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/geng/tinyml/ppq/benchmark/classification/demo.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcad02/home/geng/tinyml/ppq/benchmark/classification/demo.ipynb#ch0000009vscode-remote?line=2'>3</a>\u001b[0m target \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOpenVino\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcad02/home/geng/tinyml/ppq/benchmark/classification/demo.ipynb#ch0000009vscode-remote?line=4'>5</a>\u001b[0m model_path \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mos\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(cfg\u001b[39m.\u001b[39mOPENVINO_BASE_PATH, name)\u001b[39m}\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m{\u001b[39;00mtarget\u001b[39m}\u001b[39;00m\u001b[39m-INT8.onnx\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bcad02/home/geng/tinyml/ppq/benchmark/classification/demo.ipynb#ch0000009vscode-remote?line=6'>7</a>\u001b[0m evaluate_openvino_module_with_imagenet(model_path\u001b[39m=\u001b[39;49mmodel_path,imagenet_validation_dir\u001b[39m=\u001b[39;49mCFG_VALIDATION_DIR,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcad02/home/geng/tinyml/ppq/benchmark/classification/demo.ipynb#ch0000009vscode-remote?line=7'>8</a>\u001b[0m batchsize\u001b[39m=\u001b[39;49mCFG_BATCHSIZE,device\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/tinyml/ppq/benchmark/classification/Utilities/Imagenet/imagenet_util.py:169\u001b[0m, in \u001b[0;36mevaluate_openvino_module_with_imagenet\u001b[0;34m(model_path, imagenet_validation_dir, batchsize, device, imagenet_validation_loader, verbose)\u001b[0m\n\u001b[1;32m    164\u001b[0m model_openvino \u001b[39m=\u001b[39m openvino_executor\u001b[39m.\u001b[39mcompile_model(\n\u001b[1;32m    165\u001b[0m model \u001b[39m=\u001b[39m openvino_executor\u001b[39m.\u001b[39mread_model(model\u001b[39m=\u001b[39mmodel_path), device_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCPU\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    167\u001b[0m model_forward_function \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m input_tensor: torch\u001b[39m.\u001b[39mtensor(\u001b[39mlist\u001b[39m(\n\u001b[1;32m    168\u001b[0m     model_openvino([convert_any_to_numpy(input_tensor)])\u001b[39m.\u001b[39mvalues())[\u001b[39m0\u001b[39m])\n\u001b[0;32m--> 169\u001b[0m \u001b[39mreturn\u001b[39;00m _evaluate_any_module_with_imagenet(\n\u001b[1;32m    170\u001b[0m     model_forward_function\u001b[39m=\u001b[39;49mmodel_forward_function, batchsize\u001b[39m=\u001b[39;49mbatchsize,\n\u001b[1;32m    171\u001b[0m     device\u001b[39m=\u001b[39;49mdevice, imagenet_validation_dir\u001b[39m=\u001b[39;49mimagenet_validation_dir,\n\u001b[1;32m    172\u001b[0m     imagenet_validation_loader\u001b[39m=\u001b[39;49mimagenet_validation_loader, verbose\u001b[39m=\u001b[39;49mverbose\n\u001b[1;32m    173\u001b[0m )\n",
      "File \u001b[0;32m~/tinyml/ppq/benchmark/classification/Utilities/Imagenet/imagenet_util.py:208\u001b[0m, in \u001b[0;36m_evaluate_any_module_with_imagenet\u001b[0;34m(model_forward_function, imagenet_validation_dir, batchsize, device, imagenet_validation_loader, verbose)\u001b[0m\n\u001b[1;32m    205\u001b[0m batch_time_mark_point \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    206\u001b[0m \u001b[39m# print(\"batch_input:\",batch_input.shape)\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m batch_pred \u001b[39m=\u001b[39m model_forward_function(batch_input)\n\u001b[1;32m    209\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(batch_pred, \u001b[39mlist\u001b[39m): batch_pred \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(batch_pred)\n\u001b[1;32m    211\u001b[0m recorder[\u001b[39m'\u001b[39m\u001b[39mbatch_time\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m batch_time_mark_point)\n",
      "File \u001b[0;32m~/tinyml/ppq/benchmark/classification/Utilities/Imagenet/imagenet_util.py:168\u001b[0m, in \u001b[0;36mevaluate_openvino_module_with_imagenet.<locals>.<lambda>\u001b[0;34m(input_tensor)\u001b[0m\n\u001b[1;32m    163\u001b[0m openvino_executor \u001b[39m=\u001b[39m openvino\u001b[39m.\u001b[39mruntime\u001b[39m.\u001b[39mCore()\n\u001b[1;32m    164\u001b[0m model_openvino \u001b[39m=\u001b[39m openvino_executor\u001b[39m.\u001b[39mcompile_model(\n\u001b[1;32m    165\u001b[0m model \u001b[39m=\u001b[39m openvino_executor\u001b[39m.\u001b[39mread_model(model\u001b[39m=\u001b[39mmodel_path), device_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCPU\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    167\u001b[0m model_forward_function \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m input_tensor: torch\u001b[39m.\u001b[39mtensor(\u001b[39mlist\u001b[39m(\n\u001b[0;32m--> 168\u001b[0m     model_openvino([convert_any_to_numpy(input_tensor)])\u001b[39m.\u001b[39mvalues())[\u001b[39m0\u001b[39m])\n\u001b[1;32m    169\u001b[0m \u001b[39mreturn\u001b[39;00m _evaluate_any_module_with_imagenet(\n\u001b[1;32m    170\u001b[0m     model_forward_function\u001b[39m=\u001b[39mmodel_forward_function, batchsize\u001b[39m=\u001b[39mbatchsize,\n\u001b[1;32m    171\u001b[0m     device\u001b[39m=\u001b[39mdevice, imagenet_validation_dir\u001b[39m=\u001b[39mimagenet_validation_dir,\n\u001b[1;32m    172\u001b[0m     imagenet_validation_loader\u001b[39m=\u001b[39mimagenet_validation_loader, verbose\u001b[39m=\u001b[39mverbose\n\u001b[1;32m    173\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/ppq/lib/python3.8/site-packages/openvino/runtime/ie_api.py:195\u001b[0m, in \u001b[0;36mCompiledModel.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, inputs: Union[\u001b[39mdict\u001b[39m, \u001b[39mlist\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mdict\u001b[39m:\n\u001b[1;32m    191\u001b[0m     \u001b[39m\"\"\"Callable infer wrapper for CompiledModel.\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \n\u001b[1;32m    193\u001b[0m \u001b[39m    Take a look at `infer_new_request` for reference.\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minfer_new_request(inputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/ppq/lib/python3.8/site-packages/openvino/runtime/ie_api.py:186\u001b[0m, in \u001b[0;36mCompiledModel.infer_new_request\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minfer_new_request\u001b[39m(\u001b[39mself\u001b[39m, inputs: Union[\u001b[39mdict\u001b[39m, \u001b[39mlist\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mdict\u001b[39m:\n\u001b[1;32m    162\u001b[0m     \u001b[39m\"\"\"Infers specified input(s) in synchronous mode.\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \n\u001b[1;32m    164\u001b[0m \u001b[39m    Blocks all methods of CompiledModel while request is running.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[39m    :rtype: Dict[openvino.runtime.ConstOutput, numpy.array]\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49minfer_new_request(\n\u001b[1;32m    187\u001b[0m         {} \u001b[39mif\u001b[39;49;00m inputs \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m normalize_inputs(inputs, get_input_types(\u001b[39mself\u001b[39;49m))\n\u001b[1;32m    188\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "name = 'ResNet18'\n",
    "target = \"OpenVino\"\n",
    "\n",
    "model_path = f'{os.path.join(cfg.OPENVINO_BASE_PATH, name)}-{target}-INT8.onnx'\n",
    "\n",
    "evaluate_openvino_module_with_imagenet(model_path=model_path,imagenet_validation_dir=CFG_VALIDATION_DIR,\n",
    "batchsize=CFG_BATCHSIZE,device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "INPUT_SHAPE = (64, 3, 224, 224) # 用来确定模型输入的尺寸，好像 imagenet 都是这个尺寸\n",
    "SAMPLES = torch.rand(size=INPUT_SHAPE)\n",
    "res = torch.tensor(list(model_openvino([convert_any_to_numpy(SAMPLES)]).values())[0])   #openvino获取结推理果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.0925,  0.9155,  1.4386,  ..., -1.0462,  0.1308,  2.4848],\n",
       "        [-1.7001,  0.7847,  1.5694,  ..., -0.9155,  0.0000,  2.2233],\n",
       "        [-1.7001,  0.6539,  1.1770,  ..., -0.9155,  0.1308,  2.2233],\n",
       "        ...,\n",
       "        [-1.5694,  0.6539,  1.3078,  ..., -0.7847,  0.1308,  2.6156],\n",
       "        [-1.7001,  0.7847,  1.4386,  ..., -0.7847,  0.0000,  2.6156],\n",
       "        [-2.0925,  0.5231,  1.1770,  ..., -0.9155,  0.2616,  2.4848]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(list(res)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试TensorRT精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "libnvinfer.so.8: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/geng/tinyml/ppq/benchmark/classification/demo.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bcad02/home/geng/tinyml/ppq/benchmark/classification/demo.ipynb#ch0000003vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtrt_infer\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcad02/home/geng/tinyml/ppq/benchmark/classification/demo.ipynb#ch0000003vscode-remote?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorrt\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtrt\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcad02/home/geng/tinyml/ppq/benchmark/classification/demo.ipynb#ch0000003vscode-remote?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minfer_trt\u001b[39m(model_path: \u001b[39mstr\u001b[39m, samples: List[np\u001b[39m.\u001b[39mndarray]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[np\u001b[39m.\u001b[39mndarray]:\n",
      "File \u001b[0;32m~/tinyml/ppq/benchmark/classification/trt_infer.py:26\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpycuda\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mautoinit\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpycuda\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdriver\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mcuda\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorrt\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtrt\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     29\u001b[0m     \u001b[39m# Sometimes python does not understand FileNotFoundError\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \u001b[39mFileNotFoundError\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ppq/lib/python3.8/site-packages/tensorrt/__init__.py:67\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m lib \u001b[39min\u001b[39;00m LIBRARIES:\n\u001b[1;32m     64\u001b[0m         ctypes\u001b[39m.\u001b[39mCDLL(find_lib(lib))\n\u001b[0;32m---> 67\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mtensorrt\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m     69\u001b[0m __version__ \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m8.2.1.8\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m \u001b[39m# Provides Python's `with` syntax\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: libnvinfer.so.8: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "import trt_infer\n",
    "import tensorrt as trt\n",
    "\n",
    "def infer_trt(model_path: str, samples: List[np.ndarray]) -> List[np.ndarray]:\n",
    "    \"\"\" Run a tensorrt model with given samples\n",
    "    你需要注意我这里留了数据 IO，数据总是从 host 送往 device 的\n",
    "    如果你只关心 GPU 上的运行时间，你应该修改这个方法使得数据不发生迁移\n",
    "    \"\"\"\n",
    "    logger = trt.Logger(trt.Logger.INFO)\n",
    "    with open(model_path, 'rb') as f, trt.Runtime(logger) as runtime:\n",
    "        engine = runtime.deserialize_cuda_engine(f.read())\n",
    "\n",
    "    results = []\n",
    "    with engine.create_execution_context() as context:\n",
    "        inputs, outputs, bindings, stream = trt_infer.allocate_buffers(context.engine)\n",
    "        for sample in tqdm(samples, desc='TensorRT is running...'):\n",
    "            inputs[0].host = convert_any_to_numpy(sample)\n",
    "            [output] = trt_infer.do_inference(\n",
    "                context, bindings=bindings, inputs=inputs, \n",
    "                outputs=outputs, stream=stream, batch_size=1)\n",
    "            results.append(convert_any_to_torch_tensor(output).reshape([-1, 1000]))\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存中间PPQ graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CFG_DUMP_PATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/geng/tinyml/ppq/benchmark/classification/demo.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bcad02/home/geng/tinyml/ppq/benchmark/classification/demo.ipynb#ch0000015vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mos\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(CFG_DUMP_PATH, model_name)\u001b[39m}\u001b[39;00m\u001b[39m-PPQ-INT8.graph\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcad02/home/geng/tinyml/ppq/benchmark/classification/demo.ipynb#ch0000015vscode-remote?line=1'>2</a>\u001b[0m     ppq_ir \u001b[39m=\u001b[39m  pickle\u001b[39m.\u001b[39mload(f)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CFG_DUMP_PATH' is not defined"
     ]
    }
   ],
   "source": [
    "platform = \"OpenVino\"  #记得修改上面两个\n",
    "CFG_DUMP_PATH = '/home/geng/tinyml/ppq/benchmark/classification/'+platform+'_output'\n",
    "model_name = \"ResNet18\"\n",
    "\n",
    "with open(f\"{os.path.join(CFG_DUMP_PATH, model_name)}-PPQ-INT8.graph\",\"rb\") as f:\n",
    "    ppq_ir =  pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ppq')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e1f49b93815805eca9ccc5299d655a62f3a8d0678e274dc3dfeb518f21176dcd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
